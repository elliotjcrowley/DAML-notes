{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70bdaac8",
   "metadata": {},
   "source": [
    "# DAML4 notes\n",
    "## Week 10 - Convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4901642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch has an annoying tendancy to crash on MacOS\n",
    "# This line helps, but please just run it on Notable instead!\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4de456",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>\n",
    "\n",
    "Last week, we looked at MLPs. These take in a vector input $\\mathbf{x}\\in\\mathbb{R}^{D}$ and pass it through multiple layers. Each layer takes in some vector $\\mathbf{h}^{(l-1)}\\in\\mathbb{R}^{H_{l-1}}$, multiplies it by a matrix, adds a bias, and then applies an elementwise nonlinearity:\n",
    "\n",
    "$$\\mathbf{h}^{(l)} = g(\\mathbf{W}^{(l) }\\mathbf{h}^{(l-1)} + \\mathbf{b}^{(l)}) \\,\\,\\text{for}\\,\\,l=0,1,\\dots$$\n",
    "\n",
    "An image is represented as a 3D tensor $\\mathbf{x}\\in\\mathbb{R}^{C\\times H \\times W}$. By vectorising an image for use in an MLP we are actually throwing away lots of useful spatial information. In the lecture, we looked at convolutional neural networks (ConvNets). These take in images as 3D tensors, and use a mixture of convolutional, pooling, and dense (fully connected) layers.\n",
    "\n",
    "### Convolutional layers\n",
    "\n",
    "A convolutional layer is quite similar to a fully connected in an MLP, in that it consists of a linear transformation followed by a non-linearity. However, it takes in a 3D tensor $\\mathbf{h}^{(l-1)}\\in\\mathbb{R}^{C_{l-1}\\times H_{l-1}\\times W_{l-1}}$ instead of a vector, and the linear transformation is a **2D convolution**:\n",
    "\n",
    "$$\\mathbf{h}^{(l)} = g(\\mathbf{W}^{(l) }\\ast \\mathbf{h}^{(l-1)} + \\mathbf{b}^{(l)})\\,\\,\\text{for}\\,\\,l=0,1,\\dots$$\n",
    "\n",
    "We can use [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) from Pytorch to perform this convolution (and add a bias). Let's first create a dummy minibatch of 100 images $\\{\\mathbf{x}^{(n)}\\}_{n=0}^{100-1}$, where each image has 3 channels, height 32, and width 32 i.e. $\\mathbf{x}\\in\\mathbb{R}^{3\\times 32\\times 32}$. We can store this whole minibatch in a 4D tensor $\\mathbf{X}\\in\\mathbb{R}^{100\\times 3\\times 32\\times 32}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9452f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # The whole pytorch package\n",
    "from torch import nn  # the nn package for making neural networks\n",
    "import torch.nn.functional as F  # Useful functions\n",
    "\n",
    "# Create a minibatch of 100 dummy images, each with 3 channels, height 32 and width 32\n",
    "X = torch.rand(size=(100, 3, 32, 32))\n",
    "print(\"-------\")\n",
    "print(f\"the shape of X is {X.shape}\")\n",
    "print(\"We store minibatches of images in 4D tensors\")\n",
    "print(f\"We have {X.shape[0]} images\")\n",
    "print(f\"Each has {X.shape[1]} channels\")\n",
    "print(f\"Each has a height of {X.shape[2]}\")\n",
    "print(f\"Each has a width of {X.shape[3]}\")\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96cc378",
   "metadata": {},
   "source": [
    "Now we'll apply a (random) 2D convolution to each data point in this minibatch. Recall from the lecture that this consists of $C_{out}$ filters each of size  $C_{in}\\times k\\times k$. $C_{in}$ must match the number of input channels, and $k$ is the kernel size of the filter which is usually 3. Let's create such a convolution where $C_{out}=5$. We can then look at the shape of its weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d72c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D convolution\n",
    "# Padding pads the input with some empty pixels. Don't worry too much about this\n",
    "# It's just so the input and output have the same width and height\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=3, padding=1)\n",
    "\n",
    "print(\"-------\")\n",
    "print(f\"the shape of the conv weights is {conv.weight.shape}\")\n",
    "print(\"We store the conv weights in a 4D tensor\")\n",
    "print(f\"There are {conv.weight.shape[0]} convolutional filters\")\n",
    "print(f\"Each filter has {conv.weight.shape[1]} channels\")\n",
    "print(f\"Each filter has a height of {conv.weight.shape[2]}\")\n",
    "print(f\"Each filter has a width of {conv.weight.shape[3]}\")\n",
    "print(\"-------\")\n",
    "\n",
    "\n",
    "print(\"-------\")\n",
    "print(f\"the shape of the conv bias is {conv.bias.shape[0]}\")\n",
    "print(\"This simply adds a constant value on to each ouput channel\")\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7187c",
   "metadata": {},
   "source": [
    "Finally, let's apply Conv2D to our input and look at the ouput shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa13479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a convolution\n",
    "Y = conv(X)\n",
    "print(\"-------\")\n",
    "print(f\"the shape of the output is {Y.shape}\")\n",
    "print(f\"We have {Y.shape[0]} 3D tensors\")\n",
    "print(f\"Each has {Y.shape[1]} channels\")\n",
    "print(f\"Each has a height of {Y.shape[2]}\")\n",
    "print(f\"Each has a width of {Y.shape[3]}\")\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cc5ceb",
   "metadata": {},
   "source": [
    "### Pooling layers\n",
    "\n",
    "Pooling layers reduce the spatial input of the input. They build translation invariance into ConvNets, and allow us to effectively keep computational cost at a similar level throughout the network by increasing channels alongside decreasing spatial resolution. Let's use max pooling on our dummy input to half its spatial resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dd32da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "print(f\"Shape before pooling is {X.shape}\")\n",
    "Z = pool(X)\n",
    "print(f\"Shape after pooling is {Z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb5a9c",
   "metadata": {},
   "source": [
    "## A ConvNet for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a654bc8f",
   "metadata": {},
   "source": [
    "Now that we have covered all the constituent parts of a ConvNet, I am going to provide code that trains the ConvNet introduced in the lecture on MNIST. Make sure you are happy with how this works before attending the lab.\n",
    "\n",
    "First, let's load in MNIST, scale it to between 0 and 1, and reshape it so data points are 3D tensors (images) instead of vectors. Note that MNIST is greyscale so there is only 1 colour channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea8d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn to read in mnist\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", as_frame=False, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae9ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist.data.astype(\"float32\")\n",
    "y = mnist.target.astype(\"int64\")\n",
    "\n",
    "# Scale X to between 0 and 1\n",
    "X /= 255.0\n",
    "\n",
    "\n",
    "# X here is a 70000 times 784 matrix\n",
    "# We want to store it as images, so it should be 70000 times 1 times 28 times 28\n",
    "# The 1 is because the images are black and white.\n",
    "\n",
    "X = X.reshape(-1, 1, 28, 28)\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Check shape\n",
    "print(f\"shape of X_train is {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cedcbda",
   "metadata": {},
   "source": [
    "Now let's create the ConvNet we walked through in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85059c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # The whole pytorch package\n",
    "from torch import nn  # the nn package for making neural networks\n",
    "import torch.nn.functional as F  # Useful functions\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)  # Fix RNG\n",
    "np.random.seed(2)\n",
    "\n",
    "\n",
    "class ConvNetModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetModule, self).__init__()\n",
    "\n",
    "        self.conv0 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.maxpool0 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.linear = nn.Linear(3136, 10)  # 1600 = number channels * width * height\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h0 = torch.relu(self.conv0(x))\n",
    "        h0_pool = self.maxpool0(h0)\n",
    "\n",
    "        h1 = torch.relu(self.conv1(h0_pool))\n",
    "        h1_pool = self.maxpool1(h1)\n",
    "\n",
    "        # flatten over channel, height and width = 64 * 7 * 7 = 3136\n",
    "        phi = h1_pool.view(-1, 3136)\n",
    "\n",
    "        fx = self.linear(phi)\n",
    "\n",
    "        return fx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7292d1",
   "metadata": {},
   "source": [
    "Let's use skorch to train the ConvNet. Unfortunately, this will take a few minutes on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cfde46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45242fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "cnn = NeuralNetClassifier(\n",
    "    ConvNetModule,\n",
    "    max_epochs=5,\n",
    "    lr=0.002,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    ")\n",
    "cnn.fit(X_train, y_train);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bd0f58",
   "metadata": {},
   "source": [
    "Finally, we'll evaluate the accuracy of this ConvNet on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ccf2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cnn.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba9e684",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>\n",
    "\n",
    "#### Written by Elliot J. Crowley and &copy; The University of Edinburgh 2022-23"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
