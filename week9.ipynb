{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70bdaac8",
   "metadata": {},
   "source": [
    "# DAML4 notes\n",
    "## Week 9 - Deep neural networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4901642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch has an annoying tendancy to crash on MacOS\n",
    "# This line helps, but please just run it on Notable instead!\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4de456",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>\n",
    "\n",
    "We now move on to the topic that is responsible for most of the current hype in machine learning. When someone in the media refers to \"AI\", they usually mean a deep neural network (DNN).\n",
    "\n",
    "For classification and regression we have considered linear models of the form \n",
    "\n",
    "$$f(\\mathbf{x}) = \\mathbf{w}^\\top \\phi(\\mathbf{x}) + b$$ \n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbf{x}\\in\\mathbb{R}^D$ and $f(\\mathbf{x})\\in\\mathbb{R}^{1}$\n",
    "- $\\phi$ is a transformation that maps $\\mathbf{x}$ to a feature vector $\\phi({\\mathbf{x}})\\in\\mathbb{R}^{Z}$ \n",
    "- $\\mathbf{w}\\in \\mathbb{R}^{Z}$ and $b\\in\\mathbb{R}^1$\n",
    "\n",
    "This setup is appropriate for regression to one dimensional outputs or for binary classification. If however, we want to perform regression to multidimensional outputs or perform multiway classification then we can write a linear model more generally as:\n",
    "\n",
    "$$f(\\mathbf{x}) = \\mathbf{W} \\phi(\\mathbf{x}) + \\mathbf{b}$$ \n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbf{x}\\in\\mathbb{R}^D$ and $f(\\mathbf{x})\\in\\mathbb{R}^{K}$\n",
    "- $\\phi$ is a transformation that maps $\\mathbf{x}$ to a feature vector $\\phi({\\mathbf{x}})\\in\\mathbb{R}^{Z}$ \n",
    "- $\\mathbf{W}\\in \\mathbb{R}^{Z\\times K}$ and $b\\in\\mathbb{R}^K$\n",
    "\n",
    "The features we use are very important! You saw in the Week 7 lab that the difference between representing audio as raw waveforms versus spectrograms for classification was huge. Before DNNs became mainstream we had to  hand-engineer feature transformations for images that involved multiple stages and were incredibly hacky.\n",
    "DNNs give us a model framework where we can learn features directly on data. We effectively have a parameterised feature transformation:\n",
    "\n",
    "$$f(\\mathbf{x}) = \\mathbf{W} \\phi_{\\boldsymbol\\theta_f}(\\mathbf{x}) + \\mathbf{b}$$ \n",
    "\n",
    "For some loss $L$ with a linear model we would solve $\\underset{\\mathbf{W},\\mathbf{b}}{\\mathrm {minimise}}\\,L$. With a DNN we instead solve $\\underset{\\boldsymbol\\theta_f,\\mathbf{W},\\mathbf{b}}{\\mathrm {minimise}}\\,L$. This means we learn a feature transformation jointly with learning the linear transformation that is applied to those features.\n",
    "\n",
    "So what does $\\phi_{\\boldsymbol\\theta}(\\mathbf{x})$ look like? In the simplest DNN, the multilayer perceptron (MLP) it is a series of linear transformations, each followed by a non-linearity.\n",
    "\n",
    "## Multilayer perceptrons (MLPs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed4b38",
   "metadata": {},
   "source": [
    "Each linear transform + non-linearity pair is referred to as a layer. When counting layers we typically include the final linear transformation too. \n",
    "\n",
    "If we denote $\\mathbf{x}$ as $\\mathbf{h}^{(0)}$ then we can write an $\\mathcal{L}$ layer MLP as:\n",
    "\n",
    "- $\\mathbf{h}^{(l)} = g(\\mathbf{W}^{(l) }\\mathbf{h}^{(l-1)} + \\mathbf{b}^{(l)}) \\,\\,\\text{for}\\,\\,l=0,1,\\dots,\\mathcal{L}-2$\n",
    "- $f(\\mathbf{x})=  \\mathbf{W}^{(\\mathcal{L} -1)}\\mathbf{h}^{(\\mathcal{L}-2)} + \\mathbf{b}^{(\\mathcal{L} -1)}$\n",
    "\n",
    "We can compare this second equation to the $f(\\mathbf{x})$ above to observe our feature transformation $\\phi_{\\boldsymbol\\theta}(\\mathbf{x})$ is = $\\mathbf{h}^{(\\mathcal{L}-2)}$ and that $\\boldsymbol\\theta_f$ consists of all the weight matrices and bias vectors of the different MLP layers. We can store all our parameters into one array $\\boldsymbol\\theta= \\{\\boldsymbol\\theta_f,\\mathbf{W}^{(\\mathcal{L} -1)},\\mathbf{b}^{(\\mathcal{L} -1)}\\}$ for notational convenience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2b38e2",
   "metadata": {},
   "source": [
    "The problem of learning an MLP is now just $\\underset{\\boldsymbol\\theta}{\\mathrm {minimise}}\\ L$, which we can do with stochastic gradient descent. We use the backpropagation algorithm from the lecture to compute all the required gradients efficiently, although when we actually run the code this is all done under the hood!\n",
    "\n",
    "Without further ado, let's consider a binary classification problem and train a 2 layer MLP to solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"lines.markersize\": 7,  # Big points\n",
    "        \"font.size\": 15,  # Larger font\n",
    "        \"xtick.major.size\": 5.0,  # Bigger xticks\n",
    "        \"ytick.major.size\": 5.0,  # Bigger yticks\n",
    "    }\n",
    ")\n",
    "\n",
    "# Generate points from a circle\n",
    "\n",
    "np.random.seed(42)\n",
    "radii = np.hstack(\n",
    "    [np.random.uniform(0, 1, size=100), np.random.uniform(1.5, 3, size=100)]\n",
    ")\n",
    "theta = np.random.uniform(0, 2 * np.pi, size=200)\n",
    "\n",
    "X = np.zeros((200, 2))\n",
    "X[:, 0] = radii * np.sin(theta)\n",
    "X[:, 1] = radii * np.cos(theta)\n",
    "\n",
    "y = np.hstack([np.zeros(100), np.ones(100)])\n",
    "\n",
    "# Now let's plot our data, with a different colour for each class\n",
    "fig, ax = plt.subplots(figsize=[6, 6])\n",
    "ax.set_aspect(\"equal\", \"box\")\n",
    "ax.scatter(X[y == 0, 0], X[y == 0, 1], color=\"b\", edgecolor=\"k\")\n",
    "ax.scatter(X[y == 1, 0], X[y == 1, 1], color=\"r\", edgecolor=\"k\")\n",
    "ax.grid()\n",
    "ax.legend(['class $0$','class $1$'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f8504",
   "metadata": {},
   "source": [
    "The two layer MLP can be written as:\n",
    "\n",
    "- $\\mathbf{h}^{(1)} = g(\\mathbf{W}^{(0) }\\mathbf{x} + \\mathbf{b}^{(0)})$\n",
    "- $f(\\mathbf{x})=  \\mathbf{W}^{(1)}\\mathbf{h}^{(1)} + \\mathbf{b}^{(1)}$\n",
    "\n",
    "Each data point is 2D, and we will use 5 *neurons* in the hidden layer. This means that $\\mathbf{W}^{(0)}\\in\\mathbb{R}^{2\\times5}$ and $\\mathbf{b}^{(0)}\\in\\mathbb{R}^{5}$.\n",
    "\n",
    "For classification with MLPs, we follow the multinominal logistic regression framework in that our output is a vector where each element is the logit for each class. There is no harm doing this even when you only have two classes. We will therefore make the output 2D where the elements are the logits for class 0 and 1.\n",
    "\n",
    "It follows that $\\mathbf{W}^{(1)}\\in\\mathbb{R}^{5\\times2}$ and $\\mathbf{b}^{(1)}\\in\\mathbb{R}^{2}$. \n",
    "\n",
    "For $g$ we will use a ReLU non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea12291",
   "metadata": {},
   "source": [
    "### Defining an MLP in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a013b9e",
   "metadata": {},
   "source": [
    "[Pytorch](https://pytorch.org) is an amazing deep learning framework. I used it for the vast majority of my postdoc, and my PhD students use it all the time. It makes it really easy to define a neural network. The code below uses Pytorch to make a module that represents our 2 layer MLP: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd6556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # The whole pytorch package\n",
    "from torch import nn  # the nn package for making neural networks\n",
    "import torch.nn.functional as F  # Useful functions\n",
    "\n",
    "torch.manual_seed(1)  # Fix RNG\n",
    "np.random.seed(2)\n",
    "\n",
    "\n",
    "class MLP_Module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_Module, self).__init__()\n",
    "\n",
    "        self.dense0 = nn.Linear(2, 3)\n",
    "        self.nonlin = F.relu\n",
    "        self.dense1 = nn.Linear(3, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Including this line makes life a lot easier\n",
    "\n",
    "        X = X.float()\n",
    "\n",
    "        # Apply the two layers to the input\n",
    "        h0 = self.nonlin(self.dense0(X))\n",
    "        out = self.dense1(h0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a869530a",
   "metadata": {},
   "source": [
    "[nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) applies a linear transform (i.e. multiplies by a weight matrix and adds a bias vector) to some input vector. The weights and bias are initialised with random values, but can (and will!) be learnt.\n",
    "\n",
    "We can create an MLP object from this class and investigate some of these weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP_Module()\n",
    "\n",
    "# This is W^(0)\n",
    "print(mlp.dense0.weight)\n",
    "\n",
    "# This is b^(0)\n",
    "print(mlp.dense0.bias)\n",
    "\n",
    "# This is W^(1)\n",
    "print(mlp.dense1.weight)\n",
    "\n",
    "# This is b^(1)\n",
    "print(mlp.dense1.bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c5809",
   "metadata": {},
   "source": [
    "The magic of Pytorch (and other deep learning libraries) is their ability to do **automatic differentation**. This means we don't have to explicitly code any gradient expressions (although you will in the lab as an exercise :)).\n",
    "\n",
    "It's best to see how this works through example. We are going to create some dummy data and labels. First, we'll pass the dummy data through to get logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaaef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy array of 10 random data points\n",
    "X_dummy = torch.randn(10, 2, requires_grad=True)\n",
    "logits = mlp.forward(X_dummy)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c916f6",
   "metadata": {},
   "source": [
    "Now we can use these logits, along with some dummy labels to compute a scalar cross entropy loss. We can then call `backward` and this will compute the gradient of the loss with respect to all the parameters automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6989f79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Dummy labels\n",
    "y_dummy = torch.empty(10, dtype=torch.long).random_(2)\n",
    "output = loss(logits, y_dummy)\n",
    "output.backward()\n",
    "\n",
    "\n",
    "# This is dL/W^(0)\n",
    "print(mlp.dense0.weight.grad)\n",
    "\n",
    "# This is dL/b^(0)\n",
    "print(mlp.dense0.bias.grad)\n",
    "\n",
    "# This is dL/W^(1)\n",
    "print(mlp.dense1.weight.grad)\n",
    "\n",
    "# This is dL/b^(1)\n",
    "print(mlp.dense1.bias.grad)\n",
    "\n",
    "# Zero out gradients to prevent memory errors\n",
    "mlp.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3237f67a",
   "metadata": {},
   "source": [
    "We will be using skorch for training, so we don't even need to call `.backward()` ourself, but it is useful to know that it is happening under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85845cc7",
   "metadata": {},
   "source": [
    "### Training an MLP in skorch \n",
    "\n",
    "Pytorch is brilliant, but tends to have verbose training code, whereas we've been used to sklearn where we can just call `.fit(X,y)` on some model. \n",
    "\n",
    "Fear not! For the rest of this course we will use [skorch](https://github.com/skorch-dev/skorch) which is an sklearn-style wrapper for Pytorch that lets us train neural networks in only a few lines.\n",
    "\n",
    "To do this, we need to use skorch's [NeuralNetClassifier](https://skorch.readthedocs.io/en/stable/classifier.html#skorch.classifier.NeuralNetClassifier) with the network module we defined in Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca373cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    MLP_Module,\n",
    "    max_epochs=200,\n",
    "    lr=0.1,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    ")\n",
    "\n",
    "# Here, lr is the learning rate\n",
    "# Criterion is our loss which is cross entropy\n",
    "# Max_epochs is the number of times we cycle through the dataset during SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1842349",
   "metadata": {},
   "source": [
    "Now, we just need to convert `y` to the types skorch expects, and call `.fit`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87370978",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.int64)\n",
    "net.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b11da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(X, np.ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38534849",
   "metadata": {},
   "source": [
    "You'll notice that skorch printed some train and validation accuracies and losses, even though we only handed in training data. This is because skorch by default holds out 20% of the training data for evaluation.\n",
    "\n",
    "### Viewing the decision boundary of the MLP\n",
    "\n",
    "We can view decision boundaries as we did with sklearn before. The only requirement is that the forward method above turns the input into a float. This is because Pytorch is very particular about its input types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70855c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for plotting the decision boundary\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.inspection import DecisionBoundaryDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb2b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot our data, with a different colour for each class\n",
    "fig, ax = plt.subplots(figsize=[6, 6])\n",
    "ax.set_aspect(\"equal\", \"box\")\n",
    "ax.scatter(X[y == 0, 0], X[y == 0, 1], color=\"b\", edgecolor=\"k\")\n",
    "ax.scatter(X[y == 1, 0], X[y == 1, 1], color=\"r\", edgecolor=\"k\")\n",
    "ax.grid()\n",
    "ax.legend([\"class $0$\", \"class $1$\"])\n",
    "# Add the decision boundary\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    net,\n",
    "    X,\n",
    "    response_method=\"predict\",\n",
    "    alpha=0.3,\n",
    "    grid_resolution=300,\n",
    "    ax=ax,\n",
    "    cmap=ListedColormap([\"b\", \"r\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df06663",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>\n",
    "\n",
    "#### Written by Elliot J. Crowley and &copy; The University of Edinburgh 2022-23"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
