{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd4f525",
   "metadata": {},
   "source": [
    "# DAML4  notes\n",
    "## Week 3 - Preprocessing, PCA, clustering\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7e27d",
   "metadata": {},
   "source": [
    "## Sklearn and a quick note\n",
    "\n",
    "Sklearn (or Scikit-learn) is a high-level machine learning (ML) library for Python. It integrates with NumPy for arrays, SciPy for optimisation, and Pandas for dataframes (tabular data). It is really, really good and we will use it heavily in this course. It is high level, which lets us spend less time hacking algorithms ourselves, and more time seeing how they work.\n",
    "\n",
    "Sklearn lets us perform PCA and K-means. \"But we haven't got to the ML part of the course. What are they doing a machine learning library?\" I hear you say. Well, some people do consider them to be *unsupervised machine learning* techniques, and other people don't. \n",
    "\n",
    "\n",
    "\n",
    "Sometimes they form part of a pipeline that eventually involves some ML. For narrative convenience, we will call them **exploratory data analysis** techniques, and leave it as that.\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "### Vectors and matrices\n",
    "\n",
    "Many data analysis and machine learning techniques require us to represent our data points as vectors of real values, each of the same dimensionality $D$. In machine learning it is convention to use *column* vectors, so we will stick to this.\n",
    "\n",
    "We can refer to some arbitrary data point as $\\mathbf{x}\\in \\mathbb{R}^{D}$ and if we want to refer to specific data points in our dataset we can write $\\mathbf{x}^{(0)},\\mathbf{x}^{(1)},\\mathbf{x}^{(2)},\\dots ,\\mathbf{x}^{(N-1)}$ where the ordering isn't important as long as it is consistent. \n",
    "\n",
    "$$\\mathbf{x} =\\begin{bmatrix}\n",
    "x_0\\\\x_1\\\\\\vdots \\\\x_{D-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\quad \\quad\n",
    "\\mathbf{x}^{(n)} =\n",
    "\\begin{bmatrix}\n",
    "x_0^{(n)}\\\\x_1^{(n)}\\\\ \\vdots\\\\x_{D-1}^{(n)} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The whole dataset is the collection of these vectors $\\{\\mathbf{x}^{(n)}\\}_{n=0}^{N-1}$ and can be stored in a dataset matrix $\\mathbf{X}$. Annoyingly, it is convention to have the *rows* of this matrix representing different data points so that $\\mathbf{X}\\in \\mathbb{R}^{N\\times D}$. I tend to refer to this as a dataset matrix, but it is often referred to as a design matrix in the literature.\n",
    "\n",
    "\n",
    "$$\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x}^{(0)^\\top}\\\\\n",
    "\\mathbf{x}^{(1)^\\top}\\\\\n",
    "\\mathbf{x}^{(2)^\\top}\\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{x}^{(N-1)^\\top}\\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_0^{(0)}&x_1^{(0)}&\\dots& x_{D-1}^{(0)} \\\\\n",
    "x_0^{(1)}&x_1^{(1)}&\\dots& x_{D-1}^{(1)} \\\\\n",
    "x_0^{(2)}&x_1^{(2)}&\\dots& x_{D-1}^{(2)} \\\\\n",
    "\\dots&\\dots&\\ddots& \\vdots \\\\\n",
    "x_0^{(N-1)}&x_1^{(N-1)}&\\dots& x_{D-1}^{(N-1)} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Given some tabular data, we need to convert it into this $\\mathbf{X}$ format. If the variables in our table are all continous variables then this is very straightforward. In fact, with a table represented as a pandas dataframe it only requires a single line of code.\n",
    "\n",
    "Let's look at the iris dataset from the lecture. We will start by having it as a dataframe and then convert it into a dataset matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d541b3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get loader for the iris dataset and import pandas and numpy\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert into dataframe with the columns as the variable names\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38af8b20",
   "metadata": {},
   "source": [
    "To convert this into $\\mathbf{X}\\in \\mathbb{R}^{N\\times D}$ we just have to run a line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e2781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe into array\n",
    "X = np.array(df)\n",
    "\n",
    "# Check the shape of X\n",
    "print(f\"X has shape {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9edb87",
   "metadata": {},
   "source": [
    "### Standardisation\n",
    "\n",
    "Standardising your data is **extremely important for PCA, K-means and many machine learning algorithms.** It allows variables to be compared on a like-for-like basis so that variables with naturally large measurements don't dominate.\n",
    "\n",
    "For some of the toy examples in the lectures and notebooks where I have generated synthetic data, I haven't bothered with standardisation. There are two reasons for this:\n",
    "\n",
    "1. It becomes more fiddly to make the kind of data I want to use to illustrate a point\n",
    "2. The variables in these synthetic datasets are all at similar scales and things still works (they are  *roughly* standardised)\n",
    "\n",
    "**This will rarely be the case with real data! When in doubt, standardise your data, seriously! :)**\n",
    "\n",
    "To do this, for each variable we compute the mean and standard deviation (SD) of the measurements across all data points. For each measurement of that variable, we subtract the mean and divide by the SD.\n",
    "\n",
    "We can do this very easily in sklearn using the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of our dataset to compare stats after\n",
    "X_old = np.array(df)\n",
    "\n",
    "# Import scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Make a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the dataset. This means computing the means and STDs\n",
    "scaler.fit(X)\n",
    "\n",
    "# Finally apply the scaler to transform the data\n",
    "X = scaler.transform(X)\n",
    "\n",
    "\n",
    "# Compare stats\n",
    "print(\n",
    "    f\"Before standardisation variable means were {np.round(X_old.mean(0),2)} and SDs were {np.round(X_old.std(0),2)}\"\n",
    ")\n",
    "print(\n",
    "    f\"After standardisation variable means are {np.round(X.mean(0),2)} and SDs are {np.round(X.std(0),2)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e815757c",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "PCA takes a **standardised** dataset matrix $\\mathbf{X}\\in \\mathbb{R}^{N\\times D} $ and returns an orthornormal matrix $\\mathbf{W}\\in \\mathbb{R}^{D\\times D}$. We learnt how to compute this in the lecture. \n",
    "\n",
    "$\\mathbf{Y}=\\mathbf{XW}$ gives us a *transformed* dataset matrix $\\mathbf{Y}\\in \\mathbb{R}^{N\\times D} $ where the new dimensions $y_0,y_1,\\dots,y_{D-1}$  are linear combinations of the original dimensions $x_0,x_1,\\dots,x_{D-1}$. \n",
    "\n",
    "The significance is that there is as much variance as possible in $y_0$ so in some sense, $y_0$ best explains the data in 1D. $y_1$ is orthonormal to $y_0$ and has the next most variance and so on.\n",
    "\n",
    "Let's revisit an example for the lecture to illustrate this. If we have a rotated ellipse, then we expect PCA to rotate it so that the major axis is along $y_0$.\n",
    "\n",
    "First, let's make a dataset of points along a rotated ellipse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6400c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessing from sklearn so we can standardise in one go using X= preprocessing.scale(X)\n",
    "# Also import matplotlib for plotting\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This makes matplotlib output nice figures without much tweaking\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"lines.markersize\": 10,  # Big points\n",
    "        \"font.size\": 15,  # Larger font\n",
    "        \"savefig.dpi\": 300,  # Higher res output\n",
    "        \"savefig.format\": \"pdf\",  # PDF outputs\n",
    "        \"savefig.bbox\": \"tight\",  # remove whitespace around figure\n",
    "        \"savefig.transparent\": True,  # transparent background\n",
    "        \"xtick.major.size\": 5.0,  # Bigger xticks\n",
    "        \"ytick.major.size\": 5.0,  # Bigger yticks\n",
    "    }\n",
    ")\n",
    "\n",
    "# Generate points on an unrotated ellipse\n",
    "a = 3\n",
    "b = 1\n",
    "theta = np.linspace(0, 2 * np.pi, 50)\n",
    "x = a * np.sin(theta)\n",
    "y = b * np.cos(theta)\n",
    "X = np.vstack((x, y)).T\n",
    "\n",
    "# Rotate this by angle angle of 45 degrees anti-clockwise\n",
    "t = np.pi / 4\n",
    "R = np.array([[np.cos(t), np.sin(t)], [-np.sin(t), np.cos(t)]])\n",
    "X = X @ R\n",
    "\n",
    "# Standardise in a single line. This is an alternative to what we did in the last cell\n",
    "X = preprocessing.scale(X)\n",
    "\n",
    "# Finally, plot\n",
    "fig, ax = plt.subplots(figsize=[6,6])\n",
    "ax.set_aspect(\"equal\", \"box\")\n",
    "ax.grid()\n",
    "ax.set_xlim([-2, 2])\n",
    "ax.set_ylim([-2, 2])\n",
    "ax.scatter(X[:, 0], X[:, 1])\n",
    "ax.set_xlabel(\"$x_0$\", size=20)\n",
    "ax.set_ylabel(\"$x_1$\", size=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf64170",
   "metadata": {},
   "source": [
    "Now let's use the [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) class from sklearn to compute $\\mathbf{W}$ and then $\\mathbf{Y}=\\mathbf{XW}$. When creating an object from this class, it takes in an argument `n_components`. This refers to the number of principal components we want to keep. In this example, we want all of them so we will set this to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56abc291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA package\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a pca object\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Calling pca.fit(X) computes W behind the scenes\n",
    "pca.fit(X)\n",
    "\n",
    "# Calling pca.transform(X) then computes X@W\n",
    "Y = pca.transform(X)\n",
    "\n",
    "# Let's now plot our transformed dataset to see if it makes sense\n",
    "\n",
    "# Plot Y\n",
    "fig, ax = plt.subplots(figsize=[6,6])\n",
    "ax.set_aspect(\"equal\", \"box\")\n",
    "ax.grid()\n",
    "ax.set_xlim([-2, 2])\n",
    "ax.set_ylim([-2, 2])\n",
    "ax.scatter(Y[:, 0], Y[:, 1],color='g')\n",
    "ax.set_xlabel(\"$y_0$\", size=20)\n",
    "ax.set_ylabel(\"$y_1$\", size=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7674db93",
   "metadata": {},
   "source": [
    "Great! PCA can also be used for dimensionality reduction by only keeping a subset of the columns of $\\mathbf{W}$. You will explore this further in the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e417890",
   "metadata": {},
   "source": [
    "## K-means\n",
    "\n",
    "K-means is a clustering algorithm that assigns each point to one of $K$ clusters. It should take in a **standardised** dataset, otherwise the distance computations will be skewed by the larger dimensions.\n",
    "\n",
    "The algorithm is straightforward:\n",
    "\n",
    "- A user decides how many clusters they want\n",
    "- Cluster centres $\\{\\mathbf{c}_{k}\\}_{k=0}^{K-1}$ are created at random \n",
    "- We then alternate between assigning points to their nearest clusters, and updating the clusters as the centre of their assigned points until there are no change\n",
    "\n",
    "However, if you look at the [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) class in sklearn you can see that there are several other arguments it can take in, other than just `n_clusters`. This is because the algorithms as presented to you in lectures are usually in their most elementary form. In practice there are lots of ways to tweak an algorithm to make it more efficient, faster or more appropriate for a specific scenario or dataset, but I digress.\n",
    "\n",
    "Let's perform K-means on some synthetic data for illustration. First we will use sklearn to make a 2D dataset that consists of 4 blobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a1591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn lets us make synthetic datasets\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Let's make a dataset of 2D points split into 4 blobs\n",
    "X, _ = make_blobs(\n",
    "    n_samples=40, centers=4, n_features=2, random_state=0, cluster_std=0.4\n",
    ")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[6, 6])\n",
    "ax.set_aspect(\"equal\", \"box\")\n",
    "ax.scatter(X[:, 0], X[:, 1], color=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be8306",
   "metadata": {},
   "source": [
    "Let's run KMeans with 4 clusters on this data, and plot the clusters in 4 different colours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad9c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import k-means class\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create k-means object\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "\n",
    "# Run the k-means algorithm\n",
    "kmeans.fit(X)\n",
    "\n",
    "# We can look at kmeans.labels_ to see the assignments\n",
    "print(kmeans.labels_)\n",
    "\n",
    "# Make a colourmap\n",
    "colormap = np.array([\"r\", \"g\", \"b\", \"y\"])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[6, 6])\n",
    "ax.set_aspect(\"equal\", \"box\")\n",
    "ax.scatter(X[:, 0], X[:, 1], c=colormap[kmeans.labels_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63242c8",
   "metadata": {},
   "source": [
    "Nice! Although this algorithm is very sensible to the intial cluster placement. Also, clusters don't necessarily correspond to anything meaningful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e4751a",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>\n",
    "\n",
    "#### Written by Elliot J. Crowley and &copy; The University of Edinburgh 2022-23"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
